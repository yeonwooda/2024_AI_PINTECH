{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18220aee-a3d1-4580-8a6c-2cbb1b857034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\mldl\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\mldl\\lib\\site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in d:\\mldl\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\mldl\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
      "   ---------------------------------------- 0.0/203.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/203.0 MB 11.8 MB/s eta 0:00:18\n",
      "    --------------------------------------- 4.2/203.0 MB 12.0 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 5.5/203.0 MB 9.6 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 6.3/203.0 MB 8.4 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 7.1/203.0 MB 7.3 MB/s eta 0:00:27\n",
      "   - -------------------------------------- 9.2/203.0 MB 7.5 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 10.0/203.0 MB 7.1 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 11.0/203.0 MB 6.7 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 11.5/203.0 MB 6.4 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 12.8/203.0 MB 6.3 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 13.6/203.0 MB 6.1 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 14.9/203.0 MB 6.1 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 16.0/203.0 MB 6.1 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 16.5/203.0 MB 5.9 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 17.0/203.0 MB 5.6 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 17.3/203.0 MB 5.6 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 17.6/203.0 MB 5.2 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 17.8/203.0 MB 5.0 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 18.1/203.0 MB 4.8 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 18.1/203.0 MB 4.8 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 18.6/203.0 MB 4.3 MB/s eta 0:00:43\n",
      "   --- ------------------------------------ 19.7/203.0 MB 4.4 MB/s eta 0:00:43\n",
      "   ---- ----------------------------------- 21.8/203.0 MB 4.6 MB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 22.3/203.0 MB 4.6 MB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 23.1/203.0 MB 4.5 MB/s eta 0:00:40\n",
      "   ---- ----------------------------------- 24.4/203.0 MB 4.6 MB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 27.0/203.0 MB 4.9 MB/s eta 0:00:37\n",
      "   ----- ---------------------------------- 28.0/203.0 MB 4.9 MB/s eta 0:00:36\n",
      "   ----- ---------------------------------- 28.8/203.0 MB 4.9 MB/s eta 0:00:36\n",
      "   ------ --------------------------------- 31.5/203.0 MB 5.1 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 32.8/203.0 MB 5.2 MB/s eta 0:00:33\n",
      "   ------ --------------------------------- 35.1/203.0 MB 5.4 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 35.9/203.0 MB 5.4 MB/s eta 0:00:31\n",
      "   ------- -------------------------------- 36.4/203.0 MB 5.3 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 37.2/203.0 MB 5.2 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 38.3/203.0 MB 5.2 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 39.6/203.0 MB 5.3 MB/s eta 0:00:32\n",
      "   ------- -------------------------------- 40.1/203.0 MB 5.2 MB/s eta 0:00:32\n",
      "   -------- ------------------------------- 41.9/203.0 MB 5.3 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 44.0/203.0 MB 5.4 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 46.4/203.0 MB 5.6 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 48.0/203.0 MB 5.6 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 49.3/203.0 MB 5.6 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 49.5/203.0 MB 5.6 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 49.5/203.0 MB 5.6 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 50.3/203.0 MB 5.4 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 52.2/203.0 MB 5.4 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 54.3/203.0 MB 5.5 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 56.4/203.0 MB 5.7 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 56.6/203.0 MB 5.6 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 56.9/203.0 MB 5.5 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 57.9/203.0 MB 5.5 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 58.5/203.0 MB 5.4 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 60.0/203.0 MB 5.5 MB/s eta 0:00:27\n",
      "   ------------ --------------------------- 61.3/203.0 MB 5.5 MB/s eta 0:00:26\n",
      "   ------------ --------------------------- 63.7/203.0 MB 5.6 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 65.0/203.0 MB 5.6 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 65.8/203.0 MB 5.6 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 67.4/203.0 MB 5.6 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 67.6/203.0 MB 5.5 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 68.4/203.0 MB 5.5 MB/s eta 0:00:25\n",
      "   ------------- -------------------------- 70.0/203.0 MB 5.5 MB/s eta 0:00:25\n",
      "   -------------- ------------------------- 71.3/203.0 MB 5.6 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 73.1/203.0 MB 5.6 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 74.4/203.0 MB 5.7 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 77.6/203.0 MB 5.8 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 80.0/203.0 MB 5.9 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 83.6/203.0 MB 6.0 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 86.8/203.0 MB 6.2 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 90.2/203.0 MB 6.3 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 93.8/203.0 MB 6.5 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 95.2/203.0 MB 6.5 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 97.5/203.0 MB 6.6 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 97.8/203.0 MB 6.5 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 100.4/203.0 MB 6.6 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 103.3/203.0 MB 6.7 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 105.9/203.0 MB 6.8 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 109.3/203.0 MB 6.9 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 112.2/203.0 MB 7.0 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 112.2/203.0 MB 7.0 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 113.5/203.0 MB 6.9 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 116.7/203.0 MB 7.0 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 119.5/203.0 MB 7.1 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 122.4/203.0 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 125.3/203.0 MB 7.3 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 127.9/203.0 MB 7.3 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 130.0/203.0 MB 7.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 133.4/203.0 MB 7.5 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 136.3/203.0 MB 7.6 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 139.5/203.0 MB 7.6 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 145.8/203.0 MB 7.9 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 154.4/203.0 MB 8.3 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 165.2/203.0 MB 8.7 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 173.3/203.0 MB 9.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 178.0/203.0 MB 9.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 186.6/203.0 MB 9.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 194.5/203.0 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 197.9/203.0 MB 9.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.6/203.0 MB 10.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 203.0/203.0 MB 9.8 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 9.3 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.12.0 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0786cedc-a95e-4c53-8e13-d63da03875b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpletransformers\n",
      "  Using cached simpletransformers-0.70.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: numpy in d:\\mldl\\lib\\site-packages (from simpletransformers) (1.26.4)\n",
      "Requirement already satisfied: requests in d:\\mldl\\lib\\site-packages (from simpletransformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in d:\\mldl\\lib\\site-packages (from simpletransformers) (4.67.1)\n",
      "Requirement already satisfied: regex in d:\\mldl\\lib\\site-packages (from simpletransformers) (2024.11.6)\n",
      "Collecting transformers>=4.31.0 (from simpletransformers)\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets (from simpletransformers)\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy in d:\\mldl\\lib\\site-packages (from simpletransformers) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\mldl\\lib\\site-packages (from simpletransformers) (1.5.1)\n",
      "Collecting seqeval (from simpletransformers)\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tensorboard in d:\\mldl\\lib\\site-packages (from simpletransformers) (2.18.0)\n",
      "Collecting tensorboardx (from simpletransformers)\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas in d:\\mldl\\lib\\site-packages (from simpletransformers) (2.2.3)\n",
      "Collecting tokenizers (from simpletransformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting wandb>=0.10.32 (from simpletransformers)\n",
      "  Using cached wandb-0.19.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting streamlit (from simpletransformers)\n",
      "  Using cached streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting sentencepiece (from simpletransformers)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: filelock in d:\\mldl\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers>=4.31.0->simpletransformers)\n",
      "  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=4.31.0->simpletransformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\mldl\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (6.0.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.31.0->simpletransformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\mldl\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.1.8)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from wandb>=0.10.32->simpletransformers) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in d:\\mldl\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from wandb>=0.10.32->simpletransformers) (6.1.0)\n",
      "Collecting pydantic<3,>=2.6 (from wandb>=0.10.32->simpletransformers)\n",
      "  Using cached pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.10.32->simpletransformers)\n",
      "  Using cached sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb>=0.10.32->simpletransformers)\n",
      "  Downloading setproctitle-1.3.4-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in d:\\mldl\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (75.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\mldl\\lib\\site-packages (from requests->simpletransformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\mldl\\lib\\site-packages (from requests->simpletransformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\mldl\\lib\\site-packages (from requests->simpletransformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\mldl\\lib\\site-packages (from requests->simpletransformers) (2024.8.30)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->simpletransformers)\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->simpletransformers)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->simpletransformers)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->simpletransformers)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->simpletransformers)\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->simpletransformers)\n",
      "  Downloading aiohttp-3.11.11-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from pandas->simpletransformers) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\mldl\\lib\\site-packages (from pandas->simpletransformers) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\mldl\\lib\\site-packages (from pandas->simpletransformers) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\mldl\\lib\\site-packages (from scikit-learn->simpletransformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\mldl\\lib\\site-packages (from scikit-learn->simpletransformers) (3.5.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit->simpletransformers)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit->simpletransformers)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit->simpletransformers)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in d:\\mldl\\lib\\site-packages (from streamlit->simpletransformers) (11.0.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in d:\\mldl\\lib\\site-packages (from streamlit->simpletransformers) (13.9.4)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit->simpletransformers)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit->simpletransformers)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in d:\\mldl\\lib\\site-packages (from streamlit->simpletransformers) (4.11.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit->simpletransformers)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from streamlit->simpletransformers) (6.4.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\mldl\\lib\\site-packages (from tensorboard->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in d:\\mldl\\lib\\site-packages (from tensorboard->simpletransformers) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\mldl\\lib\\site-packages (from tensorboard->simpletransformers) (3.7)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\mldl\\lib\\site-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\mldl\\lib\\site-packages (from tensorboard->simpletransformers) (3.1.3)\n",
      "Requirement already satisfied: jinja2 in d:\\mldl\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\mldl\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit->simpletransformers)\n",
      "  Using cached narwhals-1.20.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->simpletransformers)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->simpletransformers)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\mldl\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->simpletransformers)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->simpletransformers)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets->simpletransformers)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->simpletransformers)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl.metadata (71 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb>=0.10.32->simpletransformers)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb>=0.10.32->simpletransformers)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0 (from streamlit->simpletransformers)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\mldl\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\mldl\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.3)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\mldl\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\mldl\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\mldl\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\mldl\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
      "Using cached simpletransformers-0.70.1-py3-none-any.whl (316 kB)\n",
      "Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached wandb-0.19.1-py3-none-win_amd64.whl (19.5 MB)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 992.0/992.0 kB 23.5 MB/s eta 0:00:00\n",
      "Using cached streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
      "Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.11-cp312-cp312-win_amd64.whl (437 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 11.3/25.1 MB 58.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.1 MB 71.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 58.8 MB/s eta 0:00:00\n",
      "Using cached pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 37.5 MB/s eta 0:00:00\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Using cached sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading setproctitle-1.3.4-cp312-cp312-win_amd64.whl (12 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached narwhals-1.20.1-py3-none-any.whl (262 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-win_amd64.whl (90 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16183 sha256=c1234e267a5526ea133b6afe8f6f65dda6af0955572a5ae91602b5e66ca23841\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\5f\\b8\\73\\0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
      "Successfully built seqeval\n",
      "Installing collected packages: sentencepiece, xxhash, watchdog, typing-extensions, toml, tensorboardx, tenacity, smmap, setproctitle, sentry-sdk, safetensors, pyarrow, propcache, narwhals, multidict, fsspec, frozenlist, docker-pycreds, dill, cachetools, blinker, annotated-types, aiohappyeyeballs, yarl, pydeck, pydantic-core, multiprocess, huggingface-hub, gitdb, aiosignal, tokenizers, seqeval, pydantic, gitpython, aiohttp, wandb, transformers, altair, streamlit, datasets, simpletransformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 altair-5.5.0 annotated-types-0.7.0 blinker-1.9.0 cachetools-5.5.0 datasets-3.2.0 dill-0.3.8 docker-pycreds-0.4.0 frozenlist-1.5.0 fsspec-2024.9.0 gitdb-4.0.11 gitpython-3.1.43 huggingface-hub-0.27.0 multidict-6.1.0 multiprocess-0.70.16 narwhals-1.20.1 propcache-0.2.1 pyarrow-18.1.0 pydantic-2.10.4 pydantic-core-2.27.2 pydeck-0.9.1 safetensors-0.4.5 sentencepiece-0.2.0 sentry-sdk-2.19.2 seqeval-1.2.2 setproctitle-1.3.4 simpletransformers-0.70.1 smmap-5.0.1 streamlit-1.41.1 tenacity-9.0.0 tensorboardx-2.6.2.2 tokenizers-0.21.0 toml-0.10.2 transformers-4.47.1 typing-extensions-4.12.2 wandb-0.19.1 watchdog-6.0.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1ec515-156a-4fdf-898b-9e7dc2c38ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\mldl\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df7585e5-9c53-4cf5-9cdf-b275ab9938ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = QuestionAnsweringArgs()\n",
    "model_args.train_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7389301-4d44-4d3f-a6aa-0d6a98b7a675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = QuestionAnsweringModel(\n",
    "    \"bert\", \"bert-base-multilingual-cased\", args=model_args, use_cuda=False\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a446953-ac24-4851-bd91-32cab503a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load : 파일로 작성된 문자열 또는 문자열 -> dict 형태 데이터로 변환\n",
    "# dumps : dict, list -> json 문자열로 바꿔주는\n",
    "\n",
    "with open(\"KorQuAD_v1.0_train.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "train_data = [ item for topic in train_data['data'] for item in topic['paragraphs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c8bc72d-93ab-4a40-a3a5-41018bd0eb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qas': [{'answers': [{'text': '교향곡', 'answer_start': 54}],\n",
       "    'id': '6566495-0-0',\n",
       "    'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'},\n",
       "   {'answers': [{'text': '1악장', 'answer_start': 421}],\n",
       "    'id': '6566495-0-1',\n",
       "    'question': '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?'},\n",
       "   {'answers': [{'text': '베토벤의 교향곡 9번', 'answer_start': 194}],\n",
       "    'id': '6566495-0-2',\n",
       "    'question': '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?'},\n",
       "   {'answers': [{'text': '파우스트', 'answer_start': 15}],\n",
       "    'id': '6566518-0-0',\n",
       "    'question': '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?'},\n",
       "   {'answers': [{'text': '합창교향곡', 'answer_start': 354}],\n",
       "    'id': '6566518-0-1',\n",
       "    'question': '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?'},\n",
       "   {'answers': [{'text': '1839', 'answer_start': 0}],\n",
       "    'id': '5917067-0-0',\n",
       "    'question': '바그너가 파우스트를 처음으로 읽은 년도는?'},\n",
       "   {'answers': [{'text': '파리', 'answer_start': 410}],\n",
       "    'id': '5917067-0-1',\n",
       "    'question': '바그너가 처음 교향곡 작곡을 한 장소는?'},\n",
       "   {'answers': [{'text': '드레스덴', 'answer_start': 534}],\n",
       "    'id': '5917067-0-2',\n",
       "    'question': '바그너의 1악장의 초연은 어디서 연주되었는가?'}],\n",
       "  'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'},\n",
       " {'qas': [{'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}],\n",
       "    'id': '6566495-1-0',\n",
       "    'question': '바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가?'},\n",
       "   {'answers': [{'text': '리스트', 'answer_start': 23}],\n",
       "    'id': '6566495-1-1',\n",
       "    'question': '잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가?'},\n",
       "   {'answers': [{'text': '20루이의 금', 'answer_start': 345}],\n",
       "    'id': '6566495-1-2',\n",
       "    'question': '바그너는 다시 개정된 총보를 얼마를 받고 팔았는가?'},\n",
       "   {'answers': [{'text': '리스트', 'answer_start': 23}],\n",
       "    'id': '6566518-1-0',\n",
       "    'question': '파우스트 교향곡을 부활시킨 사람은?'},\n",
       "   {'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}],\n",
       "    'id': '6566518-1-1',\n",
       "    'question': '파우스트 교향곡을 피아노 독주용으로 편곡한 사람은?'},\n",
       "   {'answers': [{'text': '리스트', 'answer_start': 23}],\n",
       "    'id': '5917067-1-0',\n",
       "    'question': '1악장을 부활시켜 연주한 사람은?'},\n",
       "   {'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}],\n",
       "    'id': '5917067-1-1',\n",
       "    'question': '파우스트 교향곡에 감탄하여 피아노곡으로 편곡한 사람은?'},\n",
       "   {'answers': [{'text': '1840년', 'answer_start': 3}],\n",
       "    'id': '5917067-1-2',\n",
       "    'question': '리스트가 바그너와 알게 된 연도는?'}],\n",
       "  'context': '한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.'},\n",
       " {'qas': [{'answers': [{'text': '주제, 동기', 'answer_start': 70}],\n",
       "    'id': '6566495-2-0',\n",
       "    'question': '서주에는 무엇이 암시되어 있는가?'},\n",
       "   {'answers': [{'text': '제1바이올린', 'answer_start': 148}],\n",
       "    'id': '6566495-2-1',\n",
       "    'question': '첫부분에는 어떤 악기를 사용해 더욱 명확하게 나타내는가?'},\n",
       "   {'answers': [{'text': '소나타 형식', 'answer_start': 272}],\n",
       "    'id': '6566495-2-2',\n",
       "    'question': '주요부는 어떤 형식으로 되어 있는가?'},\n",
       "   {'answers': [{'text': '저음 주제', 'answer_start': 102}],\n",
       "    'id': '6566518-2-0',\n",
       "    'question': '첫 부분의 주요주제를 암시하는 주제는?'},\n",
       "   {'answers': [{'text': 'D장조', 'answer_start': 409}],\n",
       "    'id': '6566518-2-1',\n",
       "    'question': '제2주제의 축소된 재현부의 조성은?'},\n",
       "   {'answers': [{'text': '4/4박자', 'answer_start': 35}],\n",
       "    'id': '5917067-2-0',\n",
       "    'question': '곡이 시작할때의 박자는?'},\n",
       "   {'answers': [{'text': '고뇌와 갈망 동기, 청춘의 사랑 동기', 'answer_start': 115}],\n",
       "    'id': '5917067-2-1',\n",
       "    'question': '이 곡의 주요 주제는?'},\n",
       "   {'answers': [{'text': 'D장조', 'answer_start': 409}],\n",
       "    'id': '5917067-2-2',\n",
       "    'question': '제 2주제에선 무슨 장조로 재현되는가?'}],\n",
       "  'context': '이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.'},\n",
       " {'qas': [{'answers': [{'text': '커닐링구스', 'answer_start': 0}],\n",
       "    'id': '6561639-0-0',\n",
       "    'question': '모든 구강기관으로 여성의 성기를 애무하는 것을 뭐라고 하는가?'},\n",
       "   {'answers': [{'text': '클리토리스', 'answer_start': 218}],\n",
       "    'id': '6561639-0-1',\n",
       "    'question': '구강성교를 할 때 중요한 감각은 무엇인가?'},\n",
       "   {'answers': [{'text': '커닐링구스', 'answer_start': 0}],\n",
       "    'id': '6561642-0-0',\n",
       "    'question': '구강기관으로 여성의 성기를 애무하는 행위를 뜻하는 용어는?'},\n",
       "   {'answers': [{'text': '클리토리스', 'answer_start': 218}],\n",
       "    'id': '6561642-0-1',\n",
       "    'question': '구강성교시 특히 어느 부위의 감각이 중요한가?'},\n",
       "   {'answers': [{'text': '구강성교', 'answer_start': 80}],\n",
       "    'id': '6561653-0-0',\n",
       "    'question': '여성의 성기를 애무하는 것을 말하는 커닐링구스를 다른 말로 무엇이라고 합니까?'},\n",
       "   {'answers': [{'text': '클리토리스', 'answer_start': 218}],\n",
       "    'id': '6561653-0-1',\n",
       "    'question': '커닐링구스는 받는 사람이 다양한 성감을 느끼는 원인이 되며 무엇의 감각이 특히 중요합니까?'}],\n",
       "  'context': '커닐링구스(커닐링거스, 쿤닐링구스, 영어: Cunnilingus)는 입술, 혀, 입 등의 모든 구강기관으로 여성의 성기를 애무하는 것을 말하며 구강성교(오럴섹스, 영어: Oral sex)라고도 한다. 보통 남성이 행하며 동성의 여성이 행하는 경우도 있다. 혀를 질에 넣거나 여성의 클리토리스, 외음부나 그 주변을 핥거나 빨아서 애무한다. 받는 사람이 다양한 성감을 느끼는 원인이 되며 특히 클리토리스의 감각이 매우 중요하다. 타액과 수성윤활제가 자주 사용되고 이것들은 부드럽게 매끄러운 자극을 가능하게 한다. 파트너의 반응에 귀를 기울이면서 손가락과 같은 다른 애무와 함께 몸 전체에 다양한 자극과 결합하여 양측이 폭넓은 즐거움을 나눌 수 있게 한다. 전희로서 하는 경우가 많지만, 오르가즘에의 도달 여부에 관계없이 커닐링구스 자체가 성행위이다.'},\n",
       " {'qas': [{'answers': [{'text': '쉐어 하이트', 'answer_start': 61}],\n",
       "    'id': '6561639-1-0',\n",
       "    'question': '오르가슴은 클리토리스에 직접적인 자극을 주는 커닐링구스를 통해 쉽게 도달할 수 있다고 여성의 성 보고서를 쓴 사람은 누구인가?'},\n",
       "   {'answers': [{'text': '개인 위생', 'answer_start': 357}],\n",
       "    'id': '6561639-1-1',\n",
       "    'question': '여성이 커닐링구스를 하기 전에 중요하게 생각하는 것은 무엇인가?'},\n",
       "   {'answers': [{'text': '80%', 'answer_start': 17}],\n",
       "    'id': '6561642-1-0',\n",
       "    'question': '일반적 통계에 따르면 오르가슴을 얻기 위해 직접적인 음핵 자극을 필요로 하는 여성은 몇 퍼센트인가?'},\n",
       "   {'answers': [{'text': '쉐어 하이트', 'answer_start': 61}],\n",
       "    'id': '6561642-1-1',\n",
       "    'question': '대부분의 여성이 커닐링구스를 통해 쉽게 오르가슴에 도달할 수 있다는 성 보고서를 작성한 학자는 누구인가?'},\n",
       "   {'answers': [{'text': '음핵 자극', 'answer_start': 39}],\n",
       "    'id': '6561653-1-0',\n",
       "    'question': '일반적인 통계로 볼 때, 여성의 80%가 오르가슴을 얻으려고 직접적인 어떤 자극을 필요로 합니까?'},\n",
       "   {'answers': [{'text': '음순', 'answer_start': 208}],\n",
       "    'id': '6561653-1-1',\n",
       "    'question': '커닐링구스를 받는 여성은 파트너의 혀가 클리토리스를 잘 자극할 수 있도록 손가락을 사용하면 직접 무엇을 분리할 수 있습니까?'}],\n",
       "  'context': '일반적인 통계에 따르면 여성의 80%가 오르가슴을 얻기 위해 직접적인 음핵 자극을 필요로 한다. 성교육 학자 쉐어 하이트의 여성의 성 보고서에 따르면 대부분의 여성의 경우 오르가슴은 클리토리스에 직접적인 자극을 주는 커닐링구스를 통해 쉽게 도달할 수 있다고 보고하고 있다. 커닐링구스를 받는 여성은 파트너의 혀가 클리토리스를 더 잘 자극할 수 있도록 손가락을 사용, 직접 음순을 분리 할 수 \\u200b\\u200b있다. 또 다리를 넓게 벌리면 파트너가 클리토리스에 구두로 쉽게 도달 할 수 있도록 외음부를 드러낼 수 있다. 파트너는 음순과 전체 생식기 영역에 대해 보다 부드럽고 집중적이지 않은 자극으로 시작하는 것이 좋다. 여성은 커닐링구스를 하기 전에 개인 위생을 중요하게 생각한다. 일부 여성들은 음모를 제거하거나 다듬어 커닐링구스 경험을 향상시키고 있다.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e88cc437-8303-4cea-a449-1ebb28c09a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████████████████████████████████| 60407/60407 [00:30<00:00, 1993.03it/s]\n",
      "add example index and unique id: 100%|██████████████████████████████████████| 60407/60407 [00:00<00:00, 1589713.27it/s]\n",
      "Epoch 1 of 1:   0%|                                                                              | 0/1 [00:00<?, ?it/s]\n",
      "Running Epoch 1 of 1:   0%|                                                                   | 0/1119 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7266:   0%|                                                    | 0/1119 [00:04<?, ?it/s]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7266:   0%|                                          | 1/1119 [00:12<3:45:30, 12.10s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7341:   0%|                                          | 1/1119 [00:17<3:45:30, 12.10s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7341:   0%|                                          | 2/1119 [00:24<3:48:58, 12.30s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7116:   0%|                                          | 2/1119 [00:28<3:48:58, 12.30s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7116:   0%|                                          | 3/1119 [00:35<3:40:52, 11.88s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.6975:   0%|                                          | 3/1119 [00:40<3:40:52, 11.88s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.6975:   0%|▏                                         | 4/1119 [00:47<3:41:52, 11.94s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7120:   0%|▏                                         | 4/1119 [00:52<3:41:52, 11.94s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7120:   0%|▏                                         | 5/1119 [00:59<3:42:16, 11.97s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7072:   0%|▏                                         | 5/1119 [01:03<3:42:16, 11.97s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.7072:   1%|▏                                         | 6/1119 [01:11<3:38:24, 11.77s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.6509:   1%|▏                                         | 6/1119 [01:16<3:38:24, 11.77s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.6509:   1%|▎                                         | 7/1119 [01:24<3:44:22, 12.11s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.6110:   1%|▎                                         | 7/1119 [01:28<3:44:22, 12.11s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.6110:   1%|▎                                         | 8/1119 [01:36<3:42:42, 12.03s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.5739:   1%|▎                                         | 8/1119 [01:40<3:42:42, 12.03s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.5739:   1%|▎                                         | 9/1119 [01:47<3:37:39, 11.77s/it]\u001b[A\n",
      "Epochs 1/1. Running Loss:    4.4711:   1%|▎                                         | 9/1119 [01:58<4:02:50, 13.13s/it]\u001b[A\n",
      "Epoch 1 of 1:   0%|                                                                              | 0/1 [01:58<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\simpletransformers\\question_answering\\question_answering_model.py:452\u001b[0m, in \u001b[0;36mQuestionAnsweringModel.train_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    448\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_cache_examples(train_examples)\n\u001b[0;32m    450\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 452\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_running_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_running_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    462\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Training of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m model complete. Saved to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel_type, output_dir\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m )\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\simpletransformers\\question_answering\\question_answering_model.py:762\u001b[0m, in \u001b[0;36mQuestionAnsweringModel.train\u001b[1;34m(self, train_dataset, output_dir, show_running_loss, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    760\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 762\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update learning rate schedule\u001b[39;00m\n\u001b[0;32m    764\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\mldl\\Lib\\site-packages\\torch\\optim\\adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    425\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train_model(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
