{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7b76d6-6c14-47e1-bc92-ad14f89f462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\mldl\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\mldl\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\mldl\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\mldl\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\mldl\\lib\\site-packages (from requests) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0e02ff-67e5-4e11-8f5e-422756d1cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트를 위한 텍스트 파일(language-never-random.txt)을 다운받습니다.\n",
    "import requests\n",
    "response = requests.get(\"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\")\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e098bbf-06e1-4f07-99e5-e5bfb25a01d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                       Language is never, ever, ever, random\\n\\n                                                               ADAM KILGARRIFF\\n\\n\\n\\n\\nAbstract\\nLanguage users never choose words randomly, and language is essentially\\nnon-random. Statistical hypothesis testing uses a null hypothesis, which\\nposits randomness. Hence, when we look at linguistic phenomena in cor-\\npora, the null hypothesis will never be true. Moreover, where there is enough\\ndata, we shall (almost) always be able to establish '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]  # 500개만 나오게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95984afd-050b-46cd-af6b-ae7b14918a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6cb75d-e748-4ee1-b428-e84d9d099342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                       Language is never, ever, ever, random\\n\\n                                                               ADAM KILGARRIFF\\n\\n\\n\\n\\nAbstract\\nLanguage users never choose words randomly, and language is essentially\\nnon-random.',\n",
       " 'Statistical hypothesis testing uses a null hypothesis, which\\nposits randomness.',\n",
       " 'Hence, when we look at linguistic phenomena in cor-\\npora, the null hypothesis will never be true.',\n",
       " 'Moreover, where there is enough\\ndata, we shall (almost) always be able to establish that it is not true.',\n",
       " 'In\\ncorpus studies, we frequently do have enough data, so the fact that a rela-\\ntion between two phenomena is demonstrably non-random, does not sup-\\nport the inference that it is not arbitrary.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ced24756-0eae-4fb4-96c9-cf4856c9a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44bfb380-e4b3-491d-bcd1-133608e0857d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['language',\n",
       "  'is',\n",
       "  'never',\n",
       "  ',',\n",
       "  'ever',\n",
       "  ',',\n",
       "  'ever',\n",
       "  ',',\n",
       "  'random',\n",
       "  'adam',\n",
       "  'kilgarriff',\n",
       "  'abstract',\n",
       "  'language',\n",
       "  'users',\n",
       "  'never',\n",
       "  'choose',\n",
       "  'words',\n",
       "  'randomly',\n",
       "  ',',\n",
       "  'and',\n",
       "  'language',\n",
       "  'is',\n",
       "  'essentially',\n",
       "  'non-random',\n",
       "  '.'],\n",
       " ['statistical',\n",
       "  'hypothesis',\n",
       "  'testing',\n",
       "  'uses',\n",
       "  'a',\n",
       "  'null',\n",
       "  'hypothesis',\n",
       "  ',',\n",
       "  'which',\n",
       "  'posits',\n",
       "  'randomness',\n",
       "  '.'],\n",
       " ['hence',\n",
       "  ',',\n",
       "  'when',\n",
       "  'we',\n",
       "  'look',\n",
       "  'at',\n",
       "  'linguistic',\n",
       "  'phenomena',\n",
       "  'in',\n",
       "  'cor-',\n",
       "  'pora',\n",
       "  ',',\n",
       "  'the',\n",
       "  'null',\n",
       "  'hypothesis',\n",
       "  'will',\n",
       "  'never',\n",
       "  'be',\n",
       "  'true',\n",
       "  '.'],\n",
       " ['moreover',\n",
       "  ',',\n",
       "  'where',\n",
       "  'there',\n",
       "  'is',\n",
       "  'enough',\n",
       "  'data',\n",
       "  ',',\n",
       "  'we',\n",
       "  'shall',\n",
       "  '(',\n",
       "  'almost',\n",
       "  ')',\n",
       "  'always',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'establish',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'true',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'corpus',\n",
       "  'studies',\n",
       "  ',',\n",
       "  'we',\n",
       "  'frequently',\n",
       "  'do',\n",
       "  'have',\n",
       "  'enough',\n",
       "  'data',\n",
       "  ',',\n",
       "  'so',\n",
       "  'the',\n",
       "  'fact',\n",
       "  'that',\n",
       "  'a',\n",
       "  'rela-',\n",
       "  'tion',\n",
       "  'between',\n",
       "  'two',\n",
       "  'phenomena',\n",
       "  'is',\n",
       "  'demonstrably',\n",
       "  'non-random',\n",
       "  ',',\n",
       "  'does',\n",
       "  'not',\n",
       "  'sup-',\n",
       "  'port',\n",
       "  'the',\n",
       "  'inference',\n",
       "  'that',\n",
       "  'it',\n",
       "  'is',\n",
       "  'not',\n",
       "  'arbitrary',\n",
       "  '.']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebb10bb1-2e97-4813-8c93-da64ed8c5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-gram 모델 설정\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b9ac88a-7a27-459f-a1a3-4cfec7b033b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, padded_sentences = padded_everygram_pipeline(3, tokenized_text) \n",
    "# train_data - everygram(max_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c190a35-5468-42af-ab9c-0d3ae8128d6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', '<s>'),\n",
       " ('<s>', '<s>', 'language'),\n",
       " ('<s>',),\n",
       " ('<s>', 'language'),\n",
       " ('<s>', 'language', 'is'),\n",
       " ('language',),\n",
       " ('language', 'is'),\n",
       " ('language', 'is', 'never'),\n",
       " ('is',),\n",
       " ('is', 'never'),\n",
       " ('is', 'never', ','),\n",
       " ('never',),\n",
       " ('never', ','),\n",
       " ('never', ',', 'ever'),\n",
       " (',',),\n",
       " (',', 'ever'),\n",
       " (',', 'ever', ','),\n",
       " ('ever',),\n",
       " ('ever', ','),\n",
       " ('ever', ',', 'ever'),\n",
       " (',',),\n",
       " (',', 'ever'),\n",
       " (',', 'ever', ','),\n",
       " ('ever',),\n",
       " ('ever', ','),\n",
       " ('ever', ',', 'random'),\n",
       " (',',),\n",
       " (',', 'random'),\n",
       " (',', 'random', 'adam'),\n",
       " ('random',),\n",
       " ('random', 'adam'),\n",
       " ('random', 'adam', 'kilgarriff'),\n",
       " ('adam',),\n",
       " ('adam', 'kilgarriff'),\n",
       " ('adam', 'kilgarriff', 'abstract'),\n",
       " ('kilgarriff',),\n",
       " ('kilgarriff', 'abstract'),\n",
       " ('kilgarriff', 'abstract', 'language'),\n",
       " ('abstract',),\n",
       " ('abstract', 'language'),\n",
       " ('abstract', 'language', 'users'),\n",
       " ('language',),\n",
       " ('language', 'users'),\n",
       " ('language', 'users', 'never'),\n",
       " ('users',),\n",
       " ('users', 'never'),\n",
       " ('users', 'never', 'choose'),\n",
       " ('never',),\n",
       " ('never', 'choose'),\n",
       " ('never', 'choose', 'words'),\n",
       " ('choose',),\n",
       " ('choose', 'words'),\n",
       " ('choose', 'words', 'randomly'),\n",
       " ('words',),\n",
       " ('words', 'randomly'),\n",
       " ('words', 'randomly', ','),\n",
       " ('randomly',),\n",
       " ('randomly', ','),\n",
       " ('randomly', ',', 'and'),\n",
       " (',',),\n",
       " (',', 'and'),\n",
       " (',', 'and', 'language'),\n",
       " ('and',),\n",
       " ('and', 'language'),\n",
       " ('and', 'language', 'is'),\n",
       " ('language',),\n",
       " ('language', 'is'),\n",
       " ('language', 'is', 'essentially'),\n",
       " ('is',),\n",
       " ('is', 'essentially'),\n",
       " ('is', 'essentially', 'non-random'),\n",
       " ('essentially',),\n",
       " ('essentially', 'non-random'),\n",
       " ('essentially', 'non-random', '.'),\n",
       " ('non-random',),\n",
       " ('non-random', '.'),\n",
       " ('non-random', '.', '</s>'),\n",
       " ('.',),\n",
       " ('.', '</s>'),\n",
       " ('.', '</s>', '</s>'),\n",
       " ('</s>',),\n",
       " ('</s>', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(list(train_data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc54e278-3bc2-44d7-bc0c-fb021ce92b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.chain object at 0x0000023113FD29E0>\n"
     ]
    }
   ],
   "source": [
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3be40ad-f229-4f9f-9dd9-ca2624b0247f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<s>',\n",
       " 'language',\n",
       " 'is',\n",
       " 'never',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'random',\n",
       " 'adam',\n",
       " 'kilgarriff',\n",
       " 'abstract',\n",
       " 'language',\n",
       " 'users',\n",
       " 'never',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly',\n",
       " ',',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'non-random',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'statistical',\n",
       " 'hypothesis',\n",
       " 'testing',\n",
       " 'uses',\n",
       " 'a',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " ',',\n",
       " 'which',\n",
       " 'posits',\n",
       " 'randomness',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'hence',\n",
       " ',',\n",
       " 'when',\n",
       " 'we',\n",
       " 'look',\n",
       " 'at',\n",
       " 'linguistic',\n",
       " 'phenomena',\n",
       " 'in',\n",
       " 'cor-',\n",
       " 'pora',\n",
       " ',',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'will',\n",
       " 'never',\n",
       " 'be',\n",
       " 'true',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'moreover',\n",
       " ',',\n",
       " 'where',\n",
       " 'there',\n",
       " 'is',\n",
       " 'enough',\n",
       " 'data',\n",
       " ',',\n",
       " 'we',\n",
       " 'shall',\n",
       " '(',\n",
       " 'almost',\n",
       " ')',\n",
       " 'always',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'establish',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'true',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'in',\n",
       " 'corpus',\n",
       " 'studies',\n",
       " ',',\n",
       " 'we',\n",
       " 'frequently',\n",
       " 'do',\n",
       " 'have',\n",
       " 'enough',\n",
       " 'data',\n",
       " ',',\n",
       " 'so',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'a',\n",
       " 'rela-',\n",
       " 'tion',\n",
       " 'between',\n",
       " 'two',\n",
       " 'phenomena',\n",
       " 'is',\n",
       " 'demonstrably',\n",
       " 'non-random',\n",
       " ',',\n",
       " 'does',\n",
       " 'not',\n",
       " 'sup-',\n",
       " 'port',\n",
       " 'the',\n",
       " 'inference',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'arbitrary',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'present',\n",
       " 'experimental',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'how',\n",
       " 'arbitrary',\n",
       " 'associations',\n",
       " 'between',\n",
       " 'word',\n",
       " 'frequencies',\n",
       " 'and',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'systematically',\n",
       " 'non-random',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'review',\n",
       " 'literature',\n",
       " 'in',\n",
       " 'which',\n",
       " 'hypothesis',\n",
       " 'test-',\n",
       " 'ing',\n",
       " 'has',\n",
       " 'been',\n",
       " 'used',\n",
       " ',',\n",
       " 'and',\n",
       " 'show',\n",
       " 'how',\n",
       " 'it',\n",
       " 'has',\n",
       " 'often',\n",
       " 'led',\n",
       " 'to',\n",
       " 'unhelpful',\n",
       " 'or',\n",
       " 'mislead-',\n",
       " 'ing',\n",
       " 'results',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'keywords',\n",
       " ':',\n",
       " '쎲쎲쎲',\n",
       " '1',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'introduction',\n",
       " 'any',\n",
       " 'two',\n",
       " 'phenomena',\n",
       " 'might',\n",
       " 'or',\n",
       " 'might',\n",
       " 'not',\n",
       " 'be',\n",
       " 'related',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " 'pos-',\n",
       " 'sibilities',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'association',\n",
       " 'is',\n",
       " 'random',\n",
       " ',',\n",
       " 'arbitrary',\n",
       " ',',\n",
       " 'motivated',\n",
       " 'or',\n",
       " 'pre-',\n",
       " 'dictable',\n",
       " '(',\n",
       " 'r',\n",
       " ',',\n",
       " 'a',\n",
       " ',',\n",
       " 'm',\n",
       " ',',\n",
       " 'p',\n",
       " ')',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'bulk',\n",
       " 'of',\n",
       " 'linguistic',\n",
       " 'questions',\n",
       " 'concern',\n",
       " 'the',\n",
       " 'dis-',\n",
       " 'tinction',\n",
       " 'between',\n",
       " 'a',\n",
       " 'and',\n",
       " 'm.',\n",
       " 'a',\n",
       " 'linguistic',\n",
       " 'account',\n",
       " 'of',\n",
       " 'a',\n",
       " 'phenomenon',\n",
       " 'gen-',\n",
       " 'erally',\n",
       " 'gives',\n",
       " 'us',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'view',\n",
       " 'the',\n",
       " 'relation',\n",
       " 'between',\n",
       " ',',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'a',\n",
       " 'verb',\n",
       " '’',\n",
       " 's',\n",
       " 'syntax',\n",
       " 'and',\n",
       " 'its',\n",
       " 'semantics',\n",
       " ',',\n",
       " 'as',\n",
       " 'motivated',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'arbitrary',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'however',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'in',\n",
       " 'general',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'model',\n",
       " 'the',\n",
       " 'a-m',\n",
       " 'distinction',\n",
       " 'mathematically',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'distinction',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'modeled',\n",
       " 'mathematically',\n",
       " 'is',\n",
       " 'between',\n",
       " 'r',\n",
       " 'and',\n",
       " 'not-r',\n",
       " ',',\n",
       " 'that',\n",
       " 'is',\n",
       " ',',\n",
       " 'between',\n",
       " 'random',\n",
       " ',',\n",
       " 'or',\n",
       " 'uncorrelated',\n",
       " ',',\n",
       " 'pairs',\n",
       " 'and',\n",
       " 'pairs',\n",
       " 'where',\n",
       " 'there',\n",
       " 'is',\n",
       " 'some',\n",
       " 'correlation',\n",
       " ',',\n",
       " 'be',\n",
       " 'it',\n",
       " 'arbitrary',\n",
       " ',',\n",
       " 'motivated',\n",
       " 'or',\n",
       " 'predictable.1',\n",
       " 'the',\n",
       " 'mechanism',\n",
       " 'here',\n",
       " 'is',\n",
       " 'hypothesis-testing',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'a',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " ',',\n",
       " 'h0',\n",
       " 'is',\n",
       " 'con-',\n",
       " 'structed',\n",
       " 'to',\n",
       " 'model',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'in',\n",
       " 'which',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'corpus',\n",
       " 'linguistics',\n",
       " 'and',\n",
       " 'linguistic',\n",
       " 'theory',\n",
       " '1⫺2',\n",
       " '(',\n",
       " '2005',\n",
       " ')',\n",
       " ',',\n",
       " '263⫺275',\n",
       " '1613-7027/05/0001⫺0263',\n",
       " '쑕',\n",
       " 'walter',\n",
       " 'de',\n",
       " 'gruyter',\n",
       " '264',\n",
       " 'a.',\n",
       " 'kilgarriff',\n",
       " 'the',\n",
       " 'two',\n",
       " 'phenomena',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mathematics',\n",
       " 'of',\n",
       " 'the',\n",
       " 'random',\n",
       " 'is',\n",
       " 'well',\n",
       " 'under-',\n",
       " 'stood',\n",
       " ',',\n",
       " 'we',\n",
       " 'can',\n",
       " 'compute',\n",
       " 'the',\n",
       " 'likelihood',\n",
       " 'of',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'given',\n",
       " 'the',\n",
       " 'data',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'if',\n",
       " 'the',\n",
       " 'likelihood',\n",
       " 'is',\n",
       " 'low',\n",
       " ',',\n",
       " 'we',\n",
       " 'reject',\n",
       " 'h0',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'for',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'is',\n",
       " 'that',\n",
       " 'language',\n",
       " 'is',\n",
       " 'not',\n",
       " 'random',\n",
       " ',',\n",
       " 'so',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'is',\n",
       " 'never',\n",
       " 'true',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'language',\n",
       " 'is',\n",
       " 'not',\n",
       " 'random',\n",
       " 'because',\n",
       " 'we',\n",
       " 'speak',\n",
       " 'or',\n",
       " 'write',\n",
       " 'with',\n",
       " 'purposes',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'do',\n",
       " 'not',\n",
       " ',',\n",
       " 'indeed',\n",
       " ',',\n",
       " 'without',\n",
       " 'computational',\n",
       " 'help',\n",
       " 'are',\n",
       " 'not',\n",
       " 'capable',\n",
       " 'of',\n",
       " ',',\n",
       " 'producing',\n",
       " 'words',\n",
       " 'or',\n",
       " 'sounds',\n",
       " 'or',\n",
       " 'sentences',\n",
       " 'or',\n",
       " 'documents',\n",
       " 'randomly',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'we',\n",
       " 'do',\n",
       " 'not',\n",
       " 'always',\n",
       " 'have',\n",
       " 'enough',\n",
       " 'data',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " ',',\n",
       " 'but',\n",
       " 'that',\n",
       " 'is',\n",
       " 'a',\n",
       " 'distinct',\n",
       " 'issue',\n",
       " ':',\n",
       " 'wherever',\n",
       " 'there',\n",
       " 'is',\n",
       " 'enough',\n",
       " 'data',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'rejected',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'using',\n",
       " 'language',\n",
       " 'corpora',\n",
       " ',',\n",
       " 'we',\n",
       " 'are',\n",
       " 'frequently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fortunate',\n",
       " 'position',\n",
       " 'of',\n",
       " 'having',\n",
       " 'very',\n",
       " 'large',\n",
       " 'quantities',\n",
       " 'of',\n",
       " 'data',\n",
       " 'at',\n",
       " 'our',\n",
       " 'disposal',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'then',\n",
       " ',',\n",
       " 'even',\n",
       " 'where',\n",
       " 'pairs',\n",
       " 'of',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'set',\n",
       " 'up',\n",
       " 'to',\n",
       " 'be',\n",
       " 'linguistically',\n",
       " 'identical',\n",
       " ',',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'is',\n",
       " 'resoundingly',\n",
       " 'defeated',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'in',\n",
       " 'section',\n",
       " '4',\n",
       " ',',\n",
       " 'we',\n",
       " 'present',\n",
       " 'an',\n",
       " 'experiment',\n",
       " 'demonstrating',\n",
       " 'this',\n",
       " 'counterintuitive',\n",
       " 'effect',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'papers',\n",
       " 'in',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'literature',\n",
       " 'where',\n",
       " 'researchers',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'an',\n",
       " 'association',\n",
       " 'was',\n",
       " 'lin-',\n",
       " 'guistically',\n",
       " 'salient',\n",
       " ',',\n",
       " 'or',\n",
       " 'used',\n",
       " 'the',\n",
       " 'confidence',\n",
       " 'with',\n",
       " 'which',\n",
       " 'h0',\n",
       " 'could',\n",
       " 'be',\n",
       " 're-',\n",
       " 'jected',\n",
       " 'as',\n",
       " 'a',\n",
       " 'measure',\n",
       " 'of',\n",
       " 'salience',\n",
       " ',',\n",
       " 'whereas',\n",
       " 'in',\n",
       " 'fact',\n",
       " 'they',\n",
       " 'were',\n",
       " 'merely',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'they',\n",
       " 'had',\n",
       " 'enough',\n",
       " 'data',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'h0',\n",
       " 'with',\n",
       " 'confidence',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'some',\n",
       " 'such',\n",
       " 'cases',\n",
       " 'are',\n",
       " 'reviewed',\n",
       " 'in',\n",
       " 'section',\n",
       " '5',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'hypothesis',\n",
       " 'testing',\n",
       " 'has',\n",
       " 'been',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'in',\n",
       " 'the',\n",
       " 'acquisition',\n",
       " 'of',\n",
       " 'subcategorization',\n",
       " 'frames',\n",
       " 'from',\n",
       " 'corpora',\n",
       " 'and',\n",
       " 'this',\n",
       " 'literature',\n",
       " 'is',\n",
       " 'considered',\n",
       " 'in',\n",
       " 'some',\n",
       " 'detail',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'alternatives',\n",
       " 'to',\n",
       " 'inappropriate',\n",
       " 'hy-',\n",
       " 'pothesis-testing',\n",
       " 'are',\n",
       " 'presented',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'before',\n",
       " 'proceeding',\n",
       " ',',\n",
       " 'may',\n",
       " 'i',\n",
       " 'clarify',\n",
       " 'that',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'is',\n",
       " 'in',\n",
       " 'no',\n",
       " 'way',\n",
       " 'critical',\n",
       " 'of',\n",
       " 'using',\n",
       " 'probability',\n",
       " 'models',\n",
       " ',',\n",
       " 'all',\n",
       " 'of',\n",
       " 'which',\n",
       " 'are',\n",
       " 'based',\n",
       " 'on',\n",
       " 'assumptions',\n",
       " 'of',\n",
       " 'randomness',\n",
       " ',',\n",
       " 'in',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'in',\n",
       " 'general',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'probability',\n",
       " 'models',\n",
       " 'have',\n",
       " 'been',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'a',\n",
       " 'large',\n",
       " 'share',\n",
       " 'of',\n",
       " 'progress',\n",
       " 'in',\n",
       " 'the',\n",
       " 'field',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'decade',\n",
       " 'and',\n",
       " 'a',\n",
       " 'half',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'randomness',\n",
       " 'assumptions',\n",
       " 'are',\n",
       " 'always',\n",
       " 'untrue',\n",
       " ',',\n",
       " 'but',\n",
       " 'that',\n",
       " 'does',\n",
       " 'not',\n",
       " 'preclude',\n",
       " 'them',\n",
       " 'from',\n",
       " 'frequently',\n",
       " 'being',\n",
       " 'useful',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'making',\n",
       " 'false',\n",
       " 'assumptions',\n",
       " 'is',\n",
       " 'often',\n",
       " 'an',\n",
       " 'ingenious',\n",
       " 'way',\n",
       " 'to',\n",
       " 'proceed',\n",
       " ';',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'arises',\n",
       " 'where',\n",
       " 'the',\n",
       " 'literal',\n",
       " 'falsity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'assumption',\n",
       " 'is',\n",
       " 'overlooked',\n",
       " ',',\n",
       " 'and',\n",
       " 'inappropri-',\n",
       " 'ate',\n",
       " 'inferences',\n",
       " 'are',\n",
       " 'drawn',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " '2',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'arbitrary',\n",
       " 'and',\n",
       " 'the',\n",
       " 'random',\n",
       " 'in',\n",
       " 'common',\n",
       " 'parlance',\n",
       " ',',\n",
       " 'random',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'are',\n",
       " 'synonyms',\n",
       " ',',\n",
       " 'with',\n",
       " 'diction-',\n",
       " 'aries',\n",
       " 'giving',\n",
       " 'near-identical',\n",
       " 'definitions',\n",
       " ':',\n",
       " 'ldoce',\n",
       " '(',\n",
       " '1995',\n",
       " ')',\n",
       " 'defines',\n",
       " 'random',\n",
       " 'as',\n",
       " 'happening',\n",
       " 'or',\n",
       " 'chosen',\n",
       " 'without',\n",
       " 'any',\n",
       " 'definite',\n",
       " 'plan',\n",
       " ',',\n",
       " 'or',\n",
       " 'pattern',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'as',\n",
       " '1',\n",
       " 'decided',\n",
       " 'or',\n",
       " 'arranged',\n",
       " 'without',\n",
       " 'any',\n",
       " 'reason',\n",
       " 'or',\n",
       " 'plan',\n",
       " ',',\n",
       " 'often',\n",
       " 'unfairly',\n",
       " '…',\n",
       " '2',\n",
       " 'happening',\n",
       " 'or',\n",
       " 'decided',\n",
       " 'by',\n",
       " 'chance',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'a',\n",
       " 'plan',\n",
       " 'language',\n",
       " 'is',\n",
       " 'never',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'ever',\n",
       " ',',\n",
       " 'random',\n",
       " '265',\n",
       " 'superficially',\n",
       " ',',\n",
       " 'randomness',\n",
       " ',',\n",
       " 'as',\n",
       " 'defined',\n",
       " 'here',\n",
       " ',',\n",
       " 'is',\n",
       " 'what',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'random',\n",
       " 'captures',\n",
       " 'and',\n",
       " 'makes',\n",
       " 'explicit',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'sense',\n",
       " 'is',\n",
       " 'defined',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'statistical',\n",
       " 'independence',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'first',\n",
       " ',',\n",
       " 'we',\n",
       " 'formalize',\n",
       " 'the',\n",
       " 'framework',\n",
       " ':',\n",
       " 'for',\n",
       " 'a',\n",
       " 'population',\n",
       " 'of',\n",
       " 'events',\n",
       " ',',\n",
       " 'the',\n",
       " 'first',\n",
       " 'phenomenon',\n",
       " 'holds',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b1ef56d-83bc-4ea1-a10b-43ddc3050b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-gram 모델 설정 -학습을 위해 MLE(Maximum Likelihood Estimation) 추정\n",
    "# MLE(Maximum Likelihood Estimation)\n",
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43ee565c-c27b-42fe-ab12-1e32881a34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLE(3) # 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58a0061a-9ca3-4fa6-bdb5-0acd65e4912a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa308507-64e5-4cf2-a959-edf065ea5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "893ea51b-a4a6-42af-ba83-129901925f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1391"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.vocab)\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0307caf7-4413-45bd-9fec-5f9c065072d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.score()\n",
    "# 만약 Vocab 집합에 포함되지 않는 단어라면 라는 특수 토큰으로 처리됩니다.\n",
    "model.score(\"is\", [\"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99e64576-e6ce-4028-9e35-ee3788c5b6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636364"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(\"never\", [\"language\", \"is\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e66ad7a-e9c5-4bfa-9d45-f04a87ac7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = list(model.vocab)\n",
    "recommends = []\n",
    "for word in vocabs:\n",
    "    score = model.score(word, [\"language\", \"is\"])\n",
    "    if score >= 0.5:\n",
    "        recommends.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86ccbf61-2098-4682-a5d8-25e2637fa6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['never']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b93d9a65-9608-4a05-ae91-abb9ae72c033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['methods',\n",
       " 'are',\n",
       " 'inevitably',\n",
       " 'noisy',\n",
       " ',',\n",
       " 'suffering',\n",
       " ',',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'from',\n",
       " 'just',\n",
       " 'those',\n",
       " 'parser',\n",
       " 'errors',\n",
       " 'that',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'process',\n",
       " 'is']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-gram 모델을 이용해서 랜덤한 새로운 텍스트를 생성\n",
    "# model.generate(토큰 갯수)\n",
    "\n",
    "model.generate(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "171d4184-22db-4a68-a855-6134de4570be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82ac7271-8d2f-4cdb-8ac3-5a0d823e1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenize = TreebankWordDetokenizer().detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39695c7d-4457-4f07-bcb3-8d3f3c8a5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, token_count):\n",
    "    sentences = model.generate(token_count)\n",
    "    content = []\n",
    "    for word in sentences:\n",
    "        if word == '<s>':\n",
    "            continue\n",
    "        if word == '</s>':\n",
    "            break\n",
    "        content.append(word)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0976b223-a48d-46a0-9899-96f376e20cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ever, random 275 carver, r. p. 1993 the case against statistical significance testing, revisited.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e7f28-d9f2-49ff-aa67-37e03a17e55a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
